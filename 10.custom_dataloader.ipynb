{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "custom_dataloader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMbFBnswooa+zWga4vJcZKx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wooridle/mlp_lecture/blob/main/custom_dataloader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "b46JM-sr3Tvg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SXfq4F8fRrLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "71HWeSkSSodv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def read_data_set(self):\n",
        "\n",
        "        all_img_files = []\n",
        "        all_labels = []\n",
        "\n",
        "        class_names = os.walk(self.data_set_path).__next__()[1][1:]\n",
        "\n",
        "        for index, class_name in enumerate(class_names):\n",
        "            label = index\n",
        "            img_dir = os.path.join(self.data_set_path, class_name)\n",
        "            img_files = os.walk(img_dir).__next__()[2]\n",
        "\n",
        "            for img_file in img_files:\n",
        "                img_file = os.path.join(img_dir, img_file)\n",
        "                img = Image.open(img_file)\n",
        "                if img is not None:\n",
        "                    all_img_files.append(img_file)\n",
        "                    all_labels.append(label)\n",
        "\n",
        "        return all_img_files, all_labels, len(all_img_files), len(class_names)\n",
        "\n",
        "    def __init__(self, data_set_path, transforms=None):\n",
        "        self.data_set_path = data_set_path\n",
        "        self.image_files_path, self.labels, self.length, self.num_classes = self.read_data_set()\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = Image.open(self.image_files_path[index])\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return {'image': image, 'label': self.labels[index]}\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length"
      ],
      "metadata": {
        "id": "JSHql8N1RxNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xs6l4Fu7RuZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "hyper_param_epoch = 20\n",
        "hyper_param_batch = 8\n",
        "hyper_param_learning_rate = 0.001\n",
        "\n",
        "transforms_train = transforms.Compose([transforms.Resize((32, 32)),\n",
        "                                       transforms.RandomRotation(10.),\n",
        "                                       transforms.ToTensor()])\n",
        "\n",
        "transforms_test = transforms.Compose([transforms.Resize((32, 32)),\n",
        "                                      transforms.ToTensor()])\n"
      ],
      "metadata": {
        "id": "Y2GW8w8vR3L4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Vtk1V-2URNWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ddwYYJ_sRQ0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_data_set = CustomImageDataset(data_set_path=\"./data/train\", transforms=transforms_train)\n",
        "train_loader = DataLoader(train_data_set, batch_size=hyper_param_batch, shuffle=True)\n",
        "\n",
        "test_data_set = CustomImageDataset(data_set_path=\"./data/test\", transforms=transforms_test)\n",
        "test_loader = DataLoader(test_data_set, batch_size=hyper_param_batch, shuffle=True)"
      ],
      "metadata": {
        "id": "lAphYJqFR5yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8mc6EFqhR9vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "if not (train_data_set.num_classes == test_data_set.num_classes):\n",
        "    print(\"error: Numbers of class in training set and test set are not equal\")\n",
        "    exit()"
      ],
      "metadata": {
        "id": "JnHCSk7JSADl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FuA5oaR_SBBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMAGE_WIDTH = 32\n",
        "IMAGE_HEIGHT = 32\n",
        "COLOR_CHANNELS = 3\n",
        "\n",
        "N_CLASSES = train_data_set.num_classes\n",
        "\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, n_hidden_nodes, n_hidden_layers):\n",
        "\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.n_hidden_nodes = n_hidden_nodes\n",
        "\n",
        "        self.n_hidden_layers = n_hidden_layers\n",
        "\n",
        "        self.fc1 = nn.Linear(IMAGE_WIDTH * IMAGE_WIDTH * COLOR_CHANNELS,\n",
        "                                   n_hidden_nodes)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.out = nn.Linear(n_hidden_nodes, N_CLASSES)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, IMAGE_WIDTH * IMAGE_WIDTH * COLOR_CHANNELS)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return F.log_softmax(self.out(x)) \n"
      ],
      "metadata": {
        "id": "AAYRkUEXSv60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FAew7fHRResf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "num_classes = train_data_set.num_classes\n",
        "\n",
        "\n",
        "hidden_nodes = 100\n",
        "layers = 1\n",
        "#### custom_model = CustomConvNet(num_classes=num_classes).to(device)\n",
        "#### H is hidden dimension; D_out is output dimension.\n",
        "\n",
        "batch_size = 64\n",
        "test_batch_size = 64\n",
        "input_size = 768\n",
        "\n",
        "N, D_in, H, D_out = batch_size, input_size, 32, 2\n",
        "\n",
        "custom_model = Net(hidden_nodes, layers)\n",
        "\n",
        "#### Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(custom_model.parameters(), lr=hyper_param_learning_rate)\n",
        "\n"
      ],
      "metadata": {
        "id": "ItWyQ2cASE9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "K9Mq9rsRSM2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for e in range(hyper_param_epoch):\n",
        "\n",
        "    for i_batch, item in enumerate(train_loader):\n",
        "\n",
        "        images = images.reshape(-1, 32*32*3)\n",
        "\n",
        "        images = item['image'].to(device)\n",
        "\n",
        "        labels = item['label'].to(device)\n",
        "\n",
        "        #### Forward pass\n",
        "\n",
        "        outputs = custom_model(images)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        #### Backward and optimize\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i_batch + 1) % hyper_param_batch == 0:\n",
        "\n",
        "            print('Epoch [{}/{}], Loss: {:.4f}'\n",
        "            \n",
        "                  .format(e + 1, hyper_param_epoch, loss.item()))"
      ],
      "metadata": {
        "id": "Okvw_cMrSOtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EQkMZsIMRjQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test the model\n",
        "\n",
        "custom_model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    correct = 0\n",
        "\n",
        "    total = 0\n",
        "\n",
        "    for item in test_loader:\n",
        "\n",
        "        images = item['image'].to(device)\n",
        "\n",
        "        labels = item['label'].to(device)\n",
        "\n",
        "        print(\"labels: \", labels)\n",
        "\n",
        "        outputs = custom_model(images)\n",
        "       \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        print(\"predicted: \", predicted)\n",
        "\n",
        "        total += len(labels)\n",
        "\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        print(correct)\n",
        "\n",
        "    print('Test Accuracy of the model on the {} test images: {} %'.format(total, 100 * correct / total))"
      ],
      "metadata": {
        "id": "5HLoQLYVSZVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP5NJrHmLGLc",
        "outputId": "ddac7876-37bb-4d41-aa30-873cff28404c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels:  tensor([1, 1, 0, 1, 1, 1, 0, 1])\n",
            "predicted:  tensor([1, 1, 0, 1, 1, 1, 0, 1])\n",
            "8\n",
            "labels:  tensor([1, 0, 1, 1, 1, 0, 0, 0])\n",
            "predicted:  tensor([1, 0, 1, 1, 1, 0, 0, 0])\n",
            "16\n",
            "labels:  tensor([0, 0, 0, 0])\n",
            "predicted:  tensor([0, 0, 0, 0])\n",
            "20\n",
            "Test Accuracy of the model on the 20 test images: 100.0 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:100: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_WIDTH = 32\n",
        "IMAGE_HEIGHT = 32\n",
        "COLOR_CHANNELS = 3\n",
        "\n",
        "N_CLASSES = train_data_set.num_classes\n",
        "\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, n_hidden_nodes, n_hidden_layers):\n",
        "        super(Net, self).__init__()\n",
        "        self.n_hidden_nodes = n_hidden_nodes\n",
        "        self.n_hidden_layers = n_hidden_layers\n",
        "\n",
        "        self.fc1 = nn.Linear(IMAGE_WIDTH * IMAGE_WIDTH * COLOR_CHANNELS,\n",
        "                                   n_hidden_nodes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.out = nn.Linear(n_hidden_nodes, N_CLASSES)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, IMAGE_WIDTH * IMAGE_WIDTH * COLOR_CHANNELS)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        return F.log_softmax(self.out(x)) \n"
      ],
      "metadata": {
        "id": "Anop7pir64VU"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_set[0]['image'].size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWSq98ZyACKZ",
        "outputId": "fd04cbfb-839f-486b-a0d1-8b1327b7f2a8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 128, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.walk('./data/train').__next__()[1][1:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uuOhg0wKFMS",
        "outputId": "68827c8d-fe59-4366-fa61-a260b1e08787"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['kagami', 'kuroko']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vaa27XGBKjyd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
